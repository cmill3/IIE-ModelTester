{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import random\n",
    "from decimal import Decimal, getcontext\n",
    "import pandas_market_calendars as mcal\n",
    "import ast\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytz\n",
    "import warnings\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score  \n",
    "\n",
    "from typing import Any, Dict, Union\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Create a calendar\n",
    "nyse = mcal.get_calendar('NYSE')\n",
    "holidays = nyse.regular_holidays\n",
    "market_holidays = holidays.holidays()\n",
    "\n",
    "index = [\"QQQ\",\"SPY\",\"IWM\",\"TLT\"]\n",
    "test_lag = [\"DAL\",\"UAL\",\"VZ\",\"T\",\"AAL\",\"AMC\"]\n",
    "train_lag = [\"AMC\"]\n",
    "bf_plus = [\"AMD\",\"NVDA\",\"PYPL\",\"GOOG\",\"GOOGL\",\"AMZN\",\"PLTR\",\"BAC\",\"AAPL\",\"NFLX\",\"ABNB\",\"CRWD\",\"SHOP\",\"FB\",\"CRM\",\n",
    "            \"MSFT\",\"F\",\"V\",\"MA\",\"JNJ\",\"DIS\",\"JPM\",\"INTC\",\"ADBE\",\"BA\",\"CVX\",\"MRNA\",\"PFE\",\"SNOW\",\"NKE\",'META',\n",
    "            'C','TGT','MMM','SQ','PANW','DAL','CSCO','UBER','SBUX']\n",
    "\n",
    "\n",
    "high_vol = ['ZM', 'UBER', 'TDOC', 'UAL', 'RCL', 'AMZN', 'ABNB', 'META', 'TSLA',\n",
    "       'LCID', 'NIO', 'RIVN', 'SQ', 'SHOP', 'DOCU', 'ROKU',\n",
    "       'TWLO', 'DDOG', 'ZS', 'NET', 'OKTA', 'ETSY', 'PINS',\n",
    "       'FUTU', 'SE', 'RBLX', 'AMD', 'NVDA', 'PYPL', 'PLTR', 'NFLX',\n",
    "       'CRWD', 'MRNA', 'SNOW', 'SOFI', 'WBD', 'ARM', 'SNAP', 'BILI',\n",
    "       'AAL', 'CCL', 'NCLH', 'LYFT', 'BIDU', 'JD', 'BABA', 'MU', 'AMAT',\n",
    "       'DKNG', 'CZR', 'VXX']\n",
    "\n",
    "mid_vol =['CMG', 'AXP', 'DAL', 'GE', 'TSM', 'GOOG', 'GOOGL', 'BAC', 'AAPL',\n",
    "       'CRM', 'MSFT', 'F', 'DIS', 'ADBE', 'BA', 'CVX', 'C', 'CAT', 'MS',\n",
    "       'WFC', 'TGT', 'INTC', 'PANW', 'ORCL', 'LOW', 'SBUX', 'NKE', 'QCOM',\n",
    "       'AVGO', 'TXN', 'MGM', 'XOM']\n",
    "\n",
    "low_vol = ['MMM', 'MRK', 'HD', 'VZ', 'V', 'MA', 'JPM',\n",
    "       'PFE', 'GS', 'IBM', 'CSCO', 'WMT', 'COST', 'QQQ', 'SPY',\n",
    "       'TLT', 'IWM']\n",
    "\n",
    "# bfog = ['QQQ','IWM','AAPL','NVDA','AMD','AMZN','SPY','MSFT','GOOG','GOOGL','C','BAC',\n",
    "#       'JPM','XOM','CVX','CSCO','INTC','DIS','IBM','BA', 'V','AXP','WMT','ADBE','F','GM']\n",
    "bf2 = ['QQQ','IWM','AAPL','NKE','SBUX','AMZN','SPY','MSFT','GOOG','GOOGL','C','BAC',\n",
    "      'JPM','XOM','CVX','CSCO','INTC','DIS','IBM','BA', 'V','AXP','WMT','ADBE','F','GM']\n",
    "\n",
    "bflv = ['QQQ','IWM','SPY','ADBE', 'GOOGL', 'GOOG', 'AMZN', 'AMD', 'AXP', 'AAPL', 'BAC', 'BA', \n",
    "        'CVX', 'CSCO', 'C', 'DIS', 'XOM', 'F', 'GM', 'INTC', 'JPM', 'MSFT', 'NFLX', 'NVDA', 'PYPL', 'V', 'IBM','ABNB']\n",
    "\n",
    "# 'SQ', 'TSM', 'DOCU', 'UBER', 'SNOW', 'PLTR','TSLA',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_training_data_local(key_list, prefix, bucket_name, start_date, end_date):\n",
    "#     df_list = []\n",
    "#     hours = [10,11,12,13,14,15]\n",
    "#     start = start_date.split(' ')[0]\n",
    "#     end = end_date.split(' ')[0]\n",
    "#     # print(file_key)\n",
    "#     for key in key_list:\n",
    "#         for hour in hours:\n",
    "#             try:\n",
    "#                 data = s3.get_object(Bucket=bucket_name, Key=f'{prefix}{key}/{hour}.csv')\n",
    "#                 df = pd.read_csv(data.get(\"Body\")) \n",
    "#                 df_list.append(df)\n",
    "#                 # df['hour'] = hour\n",
    "#             except:\n",
    "#                 continue\n",
    "\n",
    "#     data = pd.concat(df_list)\n",
    "#     data = data.loc[~data['symbol'].isin(index)]\n",
    "#     # data = data.loc[~data['symbol'].isin(laggards)]\n",
    "#     data.reset_index(drop=True, inplace=True)\n",
    "#     # data['date'] = pd.to_datetime(data['date'])\n",
    "#     # data['day_of_week'] = data['dt'].apply(lambda x: x.dayofweek)\n",
    "#     # data = data.round(3)\n",
    "#     data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "#     data.to_csv(f'/Users/charlesmiller/Documents/ALL_SYM/{start}_{end}.csv', index=False)\n",
    "\n",
    "#     return data\n",
    "\n",
    "def pull_training_data_local(end_date,start_date):\n",
    "    data = pd.read_csv(f'/Users/charlesmiller/Documents/model_tester_data/BF/2018-01-01_2023-12-23BF3.csv')\n",
    "    data['dt'] = pd.to_datetime(data['date'])\n",
    "    data = data.loc[data['dt'] <= end_date]\n",
    "    data['one_max_vol'] = (data['one_max']/data['return_vol_10D']).round(3)\n",
    "    data['three_max_vol'] = (data['three_max']/data['return_vol_10D']).round(3)\n",
    "    data['one_min_vol'] = (data['one_min']/data['return_vol_10D']).round(3)\n",
    "    data['three_min_vol'] = (data['three_min']/data['return_vol_10D']).round(3)    \n",
    "    data['one_max_vol30'] = (data['one_max']/data['return_vol_30D']).round(3)\n",
    "    data['three_max_vol30'] = (data['three_max']/data['return_vol_30D']).round(3)\n",
    "    data['one_min_vol30'] = (data['one_min']/data['return_vol_30D']).round(3)\n",
    "    data['three_min_vol30'] = (data['three_min']/data['return_vol_30D']).round(3)    \n",
    "    data['cd_vol'] = (data['close_diff']/data['return_vol_10D']).round(3)\n",
    "    data['cd_vol3'] = (data['close_diff3']/data['return_vol_10D']).round(3)\n",
    "    data['cd_vol30'] = (data['close_diff']/data['return_vol_30D']).round(3)\n",
    "    data['cd_vol330'] = (data['close_diff3']/data['return_vol_30D']).round(3)\n",
    "    # data = data.loc[data['symbol'].isin(bf2)]\n",
    "    data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def pull_validation_data_local(end_date,start_date):\n",
    "    data = pd.read_csv(f'/Users/charlesmiller/Documents/model_tester_data/BF/2018-01-01_2023-12-23BF3.csv')\n",
    "    data['dt'] = pd.to_datetime(data['date'])\n",
    "    data['one_max_vol'] = (data['one_max']/data['return_vol_10D']).round(3)\n",
    "    data['three_max_vol'] = (data['three_max']/data['return_vol_10D']).round(3)\n",
    "    data['one_min_vol'] = (data['one_min']/data['return_vol_10D']).round(3)\n",
    "    data['three_min_vol'] = (data['three_min']/data['return_vol_10D']).round(3)    \n",
    "    data['one_max_vol30'] = (data['one_max']/data['return_vol_30D']).round(3)\n",
    "    data['three_max_vol30'] = (data['three_max']/data['return_vol_30D']).round(3)\n",
    "    data['one_min_vol30'] = (data['one_min']/data['return_vol_30D']).round(3)\n",
    "    data['three_min_vol30'] = (data['three_min']/data['return_vol_30D']).round(3)    \n",
    "    data['cd_vol'] = (data['close_diff']/data['return_vol_10D']).round(3)\n",
    "    data['cd_vol3'] = (data['close_diff3']/data['return_vol_10D']).round(3)\n",
    "    data['cd_vol30'] = (data['close_diff']/data['return_vol_30D']).round(3)\n",
    "    data['cd_vol330'] = (data['close_diff3']/data['return_vol_30D']).round(3)\n",
    "    data = data.loc[data['dt'] <= end_date]\n",
    "    data = data.loc[data['dt'] >= start_date]\n",
    "    # data = data.loc[data['symbol'].isin(bf2)]\n",
    "    data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_date_list(start_date, end_date):\n",
    "    print(start_date, end_date)\n",
    "    date_diff = end_date - start_date\n",
    "    numdays = date_diff.days \n",
    "    dateList = []\n",
    "    for x in range (0, numdays):\n",
    "        temp_date = start_date + timedelta(days = x)\n",
    "        if temp_date.weekday() > 4:\n",
    "            continue\n",
    "        else:\n",
    "            dateList.append(temp_date)\n",
    "    return dateList\n",
    "\n",
    "def build_query_keys_hist():\n",
    "    start_date = datetime(2021,1,5)\n",
    "    date_diff = datetime(2022,7,29) - start_date\n",
    "    numdays = date_diff.days \n",
    "    key_list = []\n",
    "    for x in range (0, numdays):\n",
    "        temp_date = start_date + timedelta(days = x)\n",
    "        if temp_date.weekday() > 4:\n",
    "            continue\n",
    "        else:\n",
    "            date_str = temp_date.strftime('%Y-%m-%d')\n",
    "            if date_str in market_holidays:\n",
    "                continue\n",
    "            else:\n",
    "                date_str = date_str.replace(\"-\",\"/\")\n",
    "                key_list.append(date_str)\n",
    "        \n",
    "    return key_list\n",
    "    \n",
    "def build_query_keys(dates):\n",
    "    key_list = []\n",
    "    for date in dates:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        if date_str in market_holidays:\n",
    "            continue\n",
    "        else:\n",
    "            year, month, day = date_str.split('-')\n",
    "            temp = f'{year}/{month}/{day}'\n",
    "            key_list.append(temp)\n",
    "\n",
    "    return key_list\n",
    "\n",
    "def build_query_keys_validation(end_date):\n",
    "    validation_end_date = end_date + timedelta(days=7)\n",
    "    dates = build_date_list(end_date, validation_end_date)\n",
    "    key_list = []\n",
    "    for date in dates:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        if date_str in market_holidays:\n",
    "            continue\n",
    "        else:\n",
    "            year, month, day = date_str.split('-')\n",
    "            temp = f'{year}/{month}/{day}'\n",
    "            key_list.append(temp)\n",
    "\n",
    "    return key_list\n",
    "\n",
    "def build_validation_dates_local(deployment_date):\n",
    "    end_date = deployment_date + timedelta(days=5)\n",
    "    return end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_results_analyzer(predictions, y_validate, target_value):\n",
    "    result_list = []\n",
    "    counter = 0\n",
    "    predictions_series = pd.Series(predictions,name='prediction_values')\n",
    "    for x in predictions:\n",
    "        if x == 1:\n",
    "            if y_validate.iloc[counter] == 1:\n",
    "                classification_result = 0\n",
    "            else:\n",
    "                classification_result = 1\n",
    "        elif x == 0:\n",
    "            if y_validate.iloc[counter] == 0:\n",
    "                classification_result = 2\n",
    "            else: \n",
    "                classification_result = 3\n",
    "        result_list.append(classification_result)\n",
    "        counter += 1\n",
    "    three_max = pd.Series(y_validate,name='three_max')\n",
    "    df = pd.concat([pd.Series(result_list,name='classifier_performance'),predictions_series,three_max],axis=1)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    # df = pd.DataFrame([result_list, y_validate], columns=['classifier_performance', 'prediction_score'])\n",
    "\n",
    "    tp = df.loc[df['classifier_performance'] == 0]\n",
    "    fp = df.loc[df['classifier_performance'] == 1]\n",
    "    tn = df.loc[df['classifier_performance'] == 2]\n",
    "    fn = df.loc[df['classifier_performance'] == 3]\n",
    "\n",
    "    # tp_scr = tp[\"prediction_score\"].mean()\n",
    "    # fp_scr = fp[\"prediction_score\"].mean()\n",
    "    # tn_scr = tn[\"prediction_score\"].mean()\n",
    "    # fn_scr = fn[\"prediction_score\"].mean()\n",
    "\n",
    "    return len(tp), \"0\", len(fp), \"0\", len(tn), \"0\", len(fn), \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(features, dataset, validation_dataset, target_label, target_value, hyperparams):\n",
    "    dataset.loc[:, 'label'] = (dataset[target_label] > target_value).astype(int)\n",
    "    validation_dataset.loc[:, 'label'] = (validation_dataset[target_label] > target_value).astype(int)\n",
    "\n",
    "    # dataset = dataset.round(3)\n",
    "    # validation_dataset = validation_dataset.round(3)\n",
    "\n",
    "    X = dataset[features].astype(float)\n",
    "    y = dataset['label']\n",
    "\n",
    "    X_validate = validation_dataset[features].astype(float)\n",
    "    y_validate = validation_dataset['label']\n",
    "\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(subsample=hyperparams['subsample'],num_round=hyperparams['num_round'],min_child_weight=hyperparams['min_child_weight'],max_depth=hyperparams['max_depth'],learning_rate=hyperparams['learning_rate'],gamma=hyperparams['gamma'],colsample_bytree=hyperparams['colsample_bytree'],verbosity=0,objective='binary:logistic',random_state=42)\n",
    "    xgb_model.fit(X,y)\n",
    "\n",
    "    print(y.value_counts())\n",
    "    predictions = xgb_model.predict(X_validate)\n",
    "    probabilities = xgb_model.predict_proba(X_validate)\n",
    "    tp, tp_scr, fp, fp_scr, tn, tn_scr, fn, fn_scr = model_results_analyzer(predictions, y_validate, target_value)\n",
    "\n",
    "    fi = xgb_model.feature_importances_\n",
    "    fi_list = []\n",
    "    counter = 0\n",
    "    for x in features:\n",
    "        fi_list.append({x:fi[counter]})\n",
    "        counter += 1\n",
    "    print(tp,fp,tn,fn)\n",
    "    return tp, tp_scr, fp, fp_scr, tn, tn_scr, fn, fn_scr,str(fi_list), predictions, probabilities\n",
    "\n",
    "\n",
    "def train_model_TSSim(features, dataset, validation_dataset, target_label, target_value, hyperparams):\n",
    "    dataset.loc[:, 'label'] = (dataset[target_label] > target_value).astype(int)\n",
    "    validation_dataset.loc[:, 'label'] = (validation_dataset[target_label] > target_value).astype(int)\n",
    "\n",
    "    dataset = dataset.round(5)\n",
    "    validation_dataset = validation_dataset.round(3)\n",
    "\n",
    "\n",
    "    X = dataset[features].astype(float)\n",
    "    y = dataset['label']\n",
    "\n",
    "    X_validate = validation_dataset[features].astype(float)\n",
    "    y_validate = validation_dataset['label']\n",
    "    \n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(subsample=hyperparams['subsample'],num_round=hyperparams['num_round'],min_child_weight=hyperparams['min_child_weight'],max_depth=hyperparams['max_depth'],learning_rate=hyperparams['learning_rate'],gamma=hyperparams['gamma'],colsample_bytree=hyperparams['colsample_bytree'],verbosity=0,objective='binary:logistic',random_state=42)\n",
    "    xgb_model.fit(X,y)\n",
    "\n",
    "    predictions = xgb_model.predict(X_validate)\n",
    "    tp, tp_scr, fp, fp_scr, tn, tn_scr, fn, fn_scr = model_results_analyzer(predictions, y_validate, target_value)\n",
    "\n",
    "    return tp, tp_scr, fp, fp_scr, tn, tn_scr, fn, fn_scr,\"str(fi_list)\", predictions, \"probabilities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_runner_v2RM(model_name, dataset_name, title, features, target_label, target_percentile, start_date, end_date,deployment_date, feature_str, hyperparams_str, hyperparams,local_data, dataset_start_date):\n",
    "    validation_end_date = build_validation_dates_local(deployment_date)\n",
    "    validation_dataset = pull_validation_data_local(validation_end_date,deployment_date)\n",
    "    dataset = pull_training_data_local(end_date, dataset_start_date)\n",
    "    dates_list = pull_gmm_labels(deployment_date)\n",
    "    rm_dataset = dataset.loc[dataset['dt'].isin(dates_list)]\n",
    "    target_value = rm_dataset[target_label].quantile(target_percentile).round(3)\n",
    "    print(target_value)\n",
    "    print(len(rm_dataset)/len(dataset))\n",
    "    tp, tp_scr, fp, fp_scr, tn, tn_scr, fn, fn_scr, fi_list, predictions, probabilities = train_model(features, rm_dataset, validation_dataset, target_label, target_value, hyperparams)\n",
    "    response = create_dynamo_record(tp, tp_scr, fp, fp_scr, tn, tn_scr, fn, fn_scr, model_name, deployment_date, dataset_name,hyperparams_str,feature_str, f\"{target_value}+{target_label}\", fi_list)\n",
    "    validation_dataset['probabilities'] = probabilities[:,1]\n",
    "    validation_dataset['predictions'] = predictions\n",
    "    validation_dataset['target_value'] = target_value\n",
    "    validation_dataset['target_pct'] = validation_dataset['target_value'] * validation_dataset['return_vol_10D']\n",
    "    validation_csv = validation_dataset.to_csv()\n",
    "    put_response = s3.put_object(Bucket=\"icarus-research-data\", Key=f\"backtesting_data/inv_alerts/{dataset_name}/{title}/{deployment_date.strftime('%Y-%m-%d')}.csv\", Body=validation_csv)\n",
    "    return response\n",
    "\n",
    "def model_runner_v2(model_name, dataset_name, title, features, target_label, target_percentile, start_date, end_date,deployment_date, feature_str, hyperparams_str, hyperparams,local_data, dataset_start_date):\n",
    "    validation_end_date = build_validation_dates_local(deployment_date)\n",
    "    validation_dataset = pull_validation_data_local(validation_end_date,deployment_date)\n",
    "    dataset = pull_training_data_local(end_date, dataset_start_date)\n",
    "    target_value = dataset[target_label].quantile(target_percentile).round(3)\n",
    "    tp, tp_scr, fp, fp_scr, tn, tn_scr, fn, fn_scr, fi_list, predictions, probabilities = train_model(features, dataset, validation_dataset, target_label, target_value, hyperparams)\n",
    "    response = create_dynamo_record(tp, tp_scr, fp, fp_scr, tn, tn_scr, fn, fn_scr, model_name, deployment_date, dataset_name,hyperparams_str,feature_str, f\"{target_value}+{target_label}\", fi_list)\n",
    "    validation_dataset['probabilities'] = probabilities[:,1]\n",
    "    validation_dataset['predictions'] = predictions\n",
    "    validation_dataset['target_value'] = target_value\n",
    "    validation_dataset['target_pct'] = validation_dataset['target_value'] * validation_dataset['return_vol_10D']\n",
    "    validation_csv = validation_dataset.to_csv()\n",
    "    put_response = s3.put_object(Bucket=\"icarus-research-data\", Key=f\"backtesting_data/inv_alerts/{dataset_name}/{title}/{deployment_date.strftime('%Y-%m-%d')}.csv\", Body=validation_csv)\n",
    "    return response\n",
    "\n",
    "def model_runner_temporal_simulation(features, target_label, target_percentile,dataset_start_date,end_date,deployment_date,hyperparams):\n",
    "    validation_end_date = build_validation_dates_local(deployment_date)\n",
    "    dataset = pull_training_data_local(end_date, dataset_start_date)\n",
    "    validation_dataset = pull_validation_data_local(validation_end_date,deployment_date)\n",
    "    dataset.dropna(subset=[\"close_diff_deviation3\"],inplace=True)\n",
    "    validation_dataset.dropna(subset=[\"close_diff_deviation3\"],inplace=True)\n",
    "    dataset.dropna(subset=[\"close_diff_deviation\"],inplace=True)\n",
    "    validation_dataset.dropna(subset=[\"close_diff_deviation\"],inplace=True)\n",
    "    target_value = dataset[target_label].quantile(target_percentile).round(3)\n",
    "    tp, tp_scr, fp, fp_scr, tn, tn_scr, fn, fn_scr, fi_list, predictions, probabilities = train_model_TSSim(features, dataset, validation_dataset, target_label, target_value, hyperparams)\n",
    "    return tp, fp, fn, tn \n",
    "\n",
    "def model_runner_data(start_date,end_date):\n",
    "    dates = build_date_list(start_date, end_date)\n",
    "    key_list = build_query_keys(dates)\n",
    "    print(key_list[-1])\n",
    "    train_data = create_training_data_local(key_list, 'full_alerts/weekly_exp_alerts/', 'inv-alerts', 'cdvol_gainers', start_date.strftime('%Y-%m-%d %H:%M:%S'),end_date.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    val_data = create_validation_data_local(key_list, 'full_alerts/trading_symbols_alerts/', 'inv-alerts', 'cdvol_gainers', start_date.strftime('%Y-%m-%d %H:%M:%S'),end_date.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    return \"train_data\", \"val_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_percentile = 0.5\n",
    "title = 'CDBFC'\n",
    "model_name = f'{title}:TSSIM1_BF3_custHypTP{target_percentile}'\n",
    "dataset_name = f'TSSIM1_BF3_custHypTP{target_percentile}'\n",
    "hyperparams = {'subsample': 0.6, 'num_round': 1000, 'min_child_weight': 10, 'max_depth': 10, 'learning_rate': 0.1, 'gamma': 2, 'colsample_bytree': 1}\n",
    "features = ['return_vol_5D', 'volume_10DDiff', 'oneD_stddev50', 'rsi', 'volume_vol_16H', 'return_vol_8H', 'min_volume_vol_diff', 'roc3', 'daily_vol_diff30', \n",
    "            'return_vol_3D', 'daily_vol_diff_pct30', 'hour_volume_vol_diff_pct', 'SPY_5D', 'hour_vol_diff_pct', 'volume_vol_10D', 'daily_volume_vol_diff_pct30', \n",
    "            'range_vol5MA', 'volume_vol_60M', 'SPY_diff3', 'month', 'vol7', 'range_vol', 'hour_volume_vol_diff', 'roc_diff', 'range_vol_diff5', 'return_vol_120M', \n",
    "            'close_diff', 'daily_volume_vol_diff', 'SPY_3D', 'volume_vol_30M', 'vol14', 'daily_vol_diff_pct']\n",
    "\n",
    "target_label = 'three_max_vol'\n",
    "\n",
    "\n",
    "# dataset_start_date = datetime(2018,1,1,tzinfo=pytz.timezone('US/Eastern')) \n",
    "# dates_list = build_evaluation_period(datetime(2022,10,3), datetime(2023,12,23))\n",
    "\n",
    "dataset = pull_training_data_local(start_date=datetime(2018,1,1),end_date=datetime(2023,8,25))\n",
    "# validation_dataset = pull_validation_data_local(start_date=datetime(2023,8,25),end_date=datetime(2023,12,28))\n",
    "target_value = dataset[target_label].quantile(target_percentile).round(3)\n",
    "dataset.loc[:, 'label'] = (dataset[target_label] > target_value).astype(int)\n",
    "# validation_dataset.loc[:, 'label'] = (validation_dataset[target_label] > target_value).astype(int)\n",
    "\n",
    "X = dataset[features].astype(float)\n",
    "y = dataset['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_validate = validation_dataset[features].astype(float)\n",
    "# y_validate = validation_dataset['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [02:20<00:00,  3.52s/trial, best loss: -0.7903022093683074]\n",
      "100%|██████████| 40/40 [02:39<00:00,  3.98s/trial, best loss: -0.7885815745638923]\n",
      "100%|██████████| 40/40 [02:41<00:00,  4.05s/trial, best loss: -0.7918581025425125]\n",
      "100%|██████████| 40/40 [02:48<00:00,  4.21s/trial, best loss: -0.7914004869030404]\n",
      "100%|██████████| 40/40 [02:15<00:00,  3.38s/trial, best loss: -0.7938533067306108]\n",
      "100%|██████████| 40/40 [03:02<00:00,  4.55s/trial, best loss: -0.7921143673006169]\n",
      "100%|██████████| 40/40 [02:46<00:00,  4.17s/trial, best loss: -0.7921143673006169]\n"
     ]
    }
   ],
   "source": [
    "def hyperparameter_tuning(space: Dict[str, Union[float, int]], \n",
    "                    X_train: pd.DataFrame, y_train: pd.Series, \n",
    "                    X_test: pd.DataFrame, y_test: pd.Series, \n",
    "                    early_stopping_rounds: int=50,\n",
    "                    metric:callable=accuracy_score) -> Dict[str, Any]:\n",
    "\n",
    "    int_vals = ['max_depth', 'reg_alpha']\n",
    "    space = {k: (int(val) if k in int_vals else val)\n",
    "             for k,val in space.items()}\n",
    "    space['early_stopping_rounds'] = early_stopping_rounds\n",
    "    model = xgb.XGBClassifier(**space)\n",
    "    evaluation = [(X_train, y_train),\n",
    "                  (X_test, y_test)]\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=evaluation, \n",
    "              verbose=False)    \n",
    "         \n",
    "    pred = model.predict(X_test)\n",
    "    score = metric(y_test, pred)\n",
    "    return {'loss': -score, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "params = {'random_state': 42, 'eval_metric': 'aucpr'}\n",
    "\n",
    "rounds = [{'max_depth': hp.quniform('max_depth', 1, 12, 1),  # tree\n",
    "           'min_child_weight': hp.loguniform('min_child_weight', -2, 3)},\n",
    "          {'scale_pos_weight':hp.uniform('scale_pos_weight', 0, 10), # imbalanced\n",
    "           'max_delta_step':hp.uniform('max_delta_step', 0, 10)},\n",
    "          {'subsample': hp.uniform('subsample', 0.5, 1),   # stochastic\n",
    "           'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)},\n",
    "          {'reg_alpha': hp.uniform('reg_alpha', 0, 10),\n",
    "            'reg_lambda': hp.uniform('reg_lambda', 1, 10)},\n",
    "          {'gamma': hp.loguniform('gamma', -10, 10)}, # regularization\n",
    "          {'learning_rate': hp.loguniform('learning_rate', -7, 0)},\n",
    "           {'num_round': hp.uniform('num_round',100,2000)}, # boosting\n",
    "]\n",
    "\n",
    "all_trials = []\n",
    "for round in rounds:\n",
    "    params = {**params, **round}\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=lambda space: hyperparameter_tuning(space, X_train, \n",
    "                                        y_train, X_test, y_test),            \n",
    "        space=params,           \n",
    "        algo=tpe.suggest,            \n",
    "        max_evals=40,            \n",
    "        trials=trials,\n",
    "    )\n",
    "    params = {**params, **best}\n",
    "    all_trials.append(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_state': 42,\n",
       " 'eval_metric': 'aucpr',\n",
       " 'max_depth': 12.0,\n",
       " 'min_child_weight': 4.672529469417379,\n",
       " 'scale_pos_weight': 1.060066979293146,\n",
       " 'max_delta_step': 5.018301899389477,\n",
       " 'subsample': 0.9996431631282587,\n",
       " 'colsample_bytree': 0.9962040273882053,\n",
       " 'reg_alpha': 0.3892597817795965,\n",
       " 'reg_lambda': 1.9328247142651187,\n",
       " 'gamma': 0.0012512866731913645,\n",
       " 'learning_rate': 0.38634310121697707,\n",
       " 'num_round': 577.7300421207158}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-tester-vxYjrMNc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
